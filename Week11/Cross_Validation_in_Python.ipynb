{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMoPlBdz8sF4"
   },
   "source": [
    "# Improve Your Model Performance using Cross Validation  \n",
    "\n",
    "If you run your machine learning model several times, even with the same configuration, you may notice that your model performance may go up and down. Why would that happen? Since machine learning models try to approximate the data - there is always __uncertainty__ in there. \n",
    "\n",
    "We want to limit the __uncertainty__ in our models, so that _the model can produce consistent results on unseen data_. In other words, if the model uncertainty is too high, the model may produce unreliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmGyqCnF-_3H"
   },
   "source": [
    "## Why do models lose stability?\n",
    "Let’s understand this using the below snapshot illustrating the fit of various models:\n",
    "\n",
    "![Model Stability](https://www.analyticsvidhya.com/wp-content/uploads/2015/11/15.png)\n",
    "\n",
    "Here, we are trying to find the relationship between size and price. To achieve this, we have taken the following steps:\n",
    "\n",
    "1. In the first plot, you can observe high error (model fitting loosely to the data) - it is an example of “Underfitting”.\n",
    "The first plot has a high error from training data points. \n",
    "2. In the second plot, we just found the right relationship between price and size, i.e., low training error and generalization of the relationship. \n",
    "3. In the third plot, we found a relationship which has almost zero training error. This is because the relationship is developed by considering each deviation in the data point (including noise), i.e., the model is too sensitive and captures random patterns which are present only in the current dataset. This is an example of “Overfitting”. \n",
    "\n",
    "A common practice in data science competitions is to iterate over various models to find a better performing model. However, it becomes difficult to distinguish whether this improvement in score is coming because we are capturing the relationship better, or we are just over-fitting the data. To find the right answer for this question, we use validation techniques. This method helps us in achieving more generalized relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LM-9wxrSAuv2"
   },
   "source": [
    "## Before Cross Validation\n",
    "\n",
    "So far, in all but one tutorials in this class, we have been using the traditional train-test split method for validation purposes. This method is called the __(_fixed_) hold-out method__. In this method, a fixed portion of the data (e.g. _20%_) is reserved for evaluation purposes. \n",
    "\n",
    "Refresh your memory with the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gty6yjp8rft"
   },
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abBLqfdD8f6Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the wine dataset\n",
    "my_data = load_breast_cancer()\n",
    "my_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgE4BDXFCM3P"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df = pd.DataFrame(my_data.data, columns=my_data.feature_names)\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51ZHhVXdCjUb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  label  \n",
       "0          0.4601                  0.11890      0  \n",
       "1          0.2750                  0.08902      0  \n",
       "2          0.3613                  0.08758      0  \n",
       "3          0.6638                  0.17300      0  \n",
       "4          0.2364                  0.07678      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df['label'] = my_data.target\n",
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ozuEkb4Ctad"
   },
   "outputs": [],
   "source": [
    "# split the data into train/test\n",
    "# test takes up 20% of the data\n",
    "X_train, X_test,y_train, y_test\\\n",
    "    = train_test_split(my_data.data, my_data.target, test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBebu-Z_DJWF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (455,), (114, 30), (114,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYzNjeLWEane"
   },
   "source": [
    "We discussed sometimes we might want to reserve a portion of the data for model optimization purposes - that portion of the data is called the validation set. So that method is called the _three-way hold-out method_.\n",
    "\n",
    "![three way hold out](https://i.stack.imgur.com/pXAfX.png)\n",
    "\n",
    "We can also do that with `train_test_split()`. Say we want to reserve `20%` for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GpzAiCUDZun"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val\\\n",
    "   = train_test_split(X_train, y_train, test_size=0.25, random_state=2020) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSxXcdsqFwT_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((341, 30), (114, 30), (341,), (114,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xbqy7dtGGgTC"
   },
   "source": [
    "Now we can try to fit the model multiple times to observe the variance in model performances with the __hold out method__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDuMwoOAGCrz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accruracy for  0 th round training:  0.8947\n",
      "Accruracy for  1 th round training:  0.8947\n",
      "Accruracy for  2 th round training:  0.8947\n",
      "Accruracy for  3 th round training:  0.8947\n",
      "Accruracy for  4 th round training:  0.8947\n",
      "Accruracy for  5 th round training:  0.8947\n",
      "Accruracy for  6 th round training:  0.8947\n",
      "Accruracy for  7 th round training:  0.8947\n",
      "Accruracy for  8 th round training:  0.8947\n",
      "Accruracy for  9 th round training:  0.8947\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=1.0)\n",
    "for i in range(10):\n",
    "  fit = clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print('Accruracy for ', i, 'th round training: ', round(accuracy_score(y_test, y_pred), 4)) # no variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they are all the same accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h82ts6waHunF"
   },
   "source": [
    "We can observe from above there are no variances in the results - since `sklearn` will always optimize your model within the current configuration.\n",
    "\n",
    "If we want to see some variance, we need to have different training/test sets (with the same 80:20 split). We can do that via the __Repeated Holdout__ method. See the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jF2-7VuWxJNV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accruracy for  0 th round training:  0.9123\n",
      "Accruracy for  1 th round training:  0.9474\n",
      "Accruracy for  2 th round training:  0.9211\n",
      "Accruracy for  3 th round training:  0.9035\n",
      "Accruracy for  4 th round training:  0.9386\n",
      "Accruracy for  5 th round training:  0.9474\n",
      "Accruracy for  6 th round training:  0.9561\n",
      "Accruracy for  7 th round training:  0.9123\n",
      "Accruracy for  8 th round training:  0.9386\n",
      "Accruracy for  9 th round training:  0.9211\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  # remove `random_state` so we have different training/test sample in every iteration\n",
    "  X_train, X_test,y_train, y_test\\\n",
    "    = train_test_split(my_data.data, my_data.target, test_size=0.2)\n",
    "  fit = clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print('Accruracy for ', i, 'th round training: ', round(accuracy_score(y_test, y_pred), 4)) # some variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing random_state creates a difference in the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUVscJpewsU2"
   },
   "source": [
    "Now you can see some bumping ups and downs in the results (variance). \n",
    "\n",
    "So what is wrong with the hold out and the repeated holdout methods? Since you essentially is training your model in one shot, you may get a \"lucky draw' of your data (in which your model outperforms the actual), or even worse, an \"unlucky draw\" (in which your model underperforms the actual). We do not want either situation - we want a __fair estimate__ of the model performance.\n",
    "\n",
    "In other words, you want your model to be exposed to as much data as you can, so the model can learn a comprehensive pattern (not a partial image) from your data. Since we cannot use all the data for training, that is why we need __Cross Validation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qs8sjY0ZAFYL"
   },
   "source": [
    "## What is Cross Validation?\n",
    "Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. Later, you test your model on this sample before finalizing it.\n",
    "\n",
    "Here are the steps involved in k-fold cross validation:\n",
    "\n",
    "1. Split your dataset into K (roughly) equal folds, and reserve 1 fold for evaluation/optimization purposes - note these two are related but different;\n",
    "2. Train the model using the remaining (K-1) folds and the reserve sample as the test (validation) set. This will help you in gauging the effectiveness of your model’s performance. \n",
    "\n",
    "If your model delivers a positive result on validation data, go ahead with the current model. \n",
    "\n",
    "Even though k-fold cross validation is the most popular type, do not assume that it is the _only_ cross validation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIOnBIt7JO6Y"
   },
   "source": [
    "## Common Methods for Cross Validation\n",
    "\n",
    "Cross Validation (CV) is a family of sampling/model evaluation/model optimization methods. In the stats context, it is a sampling method. In the machine learning context, it is widely used for model evaluation and/or model optimization purposes. \n",
    "\n",
    "There is a recommendation that every model needs to go through CV once, either for model evaluation or model optimization purposes. \n",
    "\n",
    "Here is a list of common CV methods:\n",
    "- Leave-One-Out Cross Validation (LOOCV) (_most extreme_)\n",
    "- K-fold Cross Validation (_most popular_)\n",
    "- Repeated K-fold Cross Validation\n",
    "- Stratified K-fold Cross Validation (_best for imblanced data_)\n",
    "- Cross Validation for Time Series (_fairly popular right now_)\n",
    "\n",
    "Let's see how to implement them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMj-44tMLkE3"
   },
   "source": [
    "### Leave-One-Out Cross Validation (LOOCV)\n",
    "\n",
    "LOOCV is the most extreme CV method. In every iteration, only __one data point__ is used for testing, the remainder of the data is used for training.\n",
    "\n",
    "Pros:\n",
    "- Model is fit to almost the whole dataset; very little chance of having a \"lucky/unlucky\" draw;\n",
    "\n",
    "Cons:\n",
    "- Training is slow;\n",
    "- Variance in model performances is high.\n",
    "\n",
    "Even though `sklearn` has its own `LeaveOneOut` method, we can essentially use the `KFold()` method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nmJMbGZTGNtN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data contains 568 data points\n",
      "TEST data contains 1 data points\n",
      "Training data contains 568 data points\n",
      "TEST data contains 1 data points\n",
      "Training data contains 568 data points\n",
      "TEST data contains 1 data points\n",
      "Training data contains 568 data points\n",
      "TEST data contains 1 data points\n"
     ]
    }
   ],
   "source": [
    "X = my_data.data\n",
    "\n",
    "kf = KFold(n_splits=len(X)) # split the data \n",
    "\n",
    "# look at the first 3 iterations\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "  print(\"Training data contains\", len(train_index), \"data points\")\n",
    "  print(\"TEST data contains\", len(test_index), \"data points\")     \n",
    "  i += 1\n",
    "  if i > 3:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Avg-k0jOE9w"
   },
   "source": [
    "We can see in every iteration, the training dataset contains $569 - 1 = 568$ instances, and the test set contains $1$ data point. \n",
    "\n",
    "Even though we can implement the LOOCV in `sklearn`, it is not well supported in it. So we will stop here and move on to the next method, K-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlBKU3pJUJPc"
   },
   "source": [
    "### K-fold Cross Validation\n",
    "\n",
    "This is the most popular method in the context of CV.\n",
    "\n",
    "Pros:\n",
    "- Most balanced method;\n",
    "- Can be used for both model evaluation and optmization purposes\n",
    "\n",
    "Cons:\n",
    "- if dataset is too small and K is too large, model might underfit\n",
    "- if k is too small, model may overfit\n",
    "\n",
    "For __evaluation purposes__, we can simply use the `cross_val_score` method.\n",
    "- it takes the model, features, target, and K as function parameters\n",
    "- by default the returned value is the accuracy score (e.g. classification accuracy for our model)\n",
    "\n",
    "Below code performs a 5-fold CV using the `SVC` model above on our data - you can see five different _accuracy_ scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcCiWrEaOBWd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85087719, 0.89473684, 0.92982456, 0.94736842, 0.9380531 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = my_data.target\n",
    "cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgX8_8ukPNYP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85263158, 0.93157895, 0.94708995])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change k to 3\n",
    "cross_val_score(clf, X, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kevx4sjmaTD-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89473684, 0.84210526, 0.89473684, 0.92982456, 0.92982456,\n",
       "       0.92982456, 0.94736842, 0.92982456, 0.92982456, 0.91071429])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change k to 10\n",
    "cross_val_score(clf, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Qi1S5yPaaQ7"
   },
   "source": [
    "If you want the final score of the model, usually we will use the __average__ across the `k` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tetH2vZ0aXA2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final model accuracy: 0.9138784461152882\n"
     ]
    }
   ],
   "source": [
    "print('final model accuracy:', cross_val_score(clf, X, y, cv=10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KV03Cf1ykyEe"
   },
   "source": [
    "You can also test how much the variance is in the results, you can check the _standard deviation_ of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEEyfxsLlAWy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model accuracy variance: 0.02878745403168186\n"
     ]
    }
   ],
   "source": [
    "print('model accuracy variance:', np.std(cross_val_score(clf, X, y, cv=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohqS71-ubCbZ"
   },
   "source": [
    "You can also specify using different metrics. For instance, we may want to focus on the _f1-score_ or the _ROC/AUC_ metric. All supported scoring metrics are listed [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "\n",
    "We can do that using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WpaIoGw-a-5j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92105263, 0.88311688, 0.91891892, 0.94736842, 0.94736842,\n",
       "       0.94594595, 0.95890411, 0.94594595, 0.94444444, 0.93333333])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, y, cv=10, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC83Busnb-yx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96753247, 0.95324675, 0.98412698, 0.96957672, 0.98809524,\n",
       "       0.98015873, 0.97883598, 0.96296296, 0.98412698, 0.99047619])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, y, cv=10, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pixwJKlKdJoe"
   },
   "source": [
    "The `cross_val_score` method is a shortcut for model evaluation purposes. The regular method is `KFold` - if we want to use the K-fold CV for model optimization purposes, we can use `KFold`, or specific methods for hyperparameter tuning that we will see next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdEIdGfvc0Fv"
   },
   "source": [
    "### Repeated K-fold Cross Validation\n",
    "\n",
    "Repeated K-fold CV is basically conducting the K-Fold cross validation `i` times. See the comparison below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGxs_9JJeylL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4],\n",
       "       [3, 0],\n",
       "       [1, 1],\n",
       "       [2, 0],\n",
       "       [1, 1],\n",
       "       [3, 1],\n",
       "       [1, 0],\n",
       "       [3, 2],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create some synthetic data for illustration\n",
    "X_data = np.random.randint(5, size=(9, 2))\n",
    "X_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKNg2Y-TgEiA"
   },
   "source": [
    "Regular K-fold CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hewgn3dWfTBy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "[[2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[2 0]\n",
      " [1 1]\n",
      " [3 1]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]]\n",
      "Test:\n",
      "[[1 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=3, random_state=2020)\n",
    "for train_index, test_index in kf.split(X_data):\n",
    "      print(\"Train:\")\n",
    "      print(X_data[train_index])\n",
    "      print(\"Test:\")\n",
    "      print(X_data[test_index])\n",
    "      print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78dI4KwrgH9r"
   },
   "source": [
    "Repeated K-fold CV:\n",
    "Note that between the repeat, the data is variant (not simple repeat, repeat with randomness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GSjOSAxlgCsX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [3 2]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[3 0]\n",
      " [3 1]\n",
      " [1 0]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[3 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 2]]\n",
      "Test:\n",
      "[[1 4]\n",
      " [2 0]\n",
      " [2 4]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[1 1]\n",
      " [3 1]\n",
      " [1 0]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[3 0]\n",
      " [1 1]\n",
      " [2 0]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[3 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]]\n",
      "Test:\n",
      "[[1 4]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 2]]\n",
      "Test:\n",
      "[[3 0]\n",
      " [1 1]\n",
      " [2 4]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[3 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[1 4]\n",
      " [1 1]\n",
      " [3 2]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[2 0]\n",
      " [3 1]\n",
      " [1 0]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[3 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 2]]\n",
      "Test:\n",
      "[[1 4]\n",
      " [2 0]\n",
      " [2 4]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[1 1]\n",
      " [3 1]\n",
      " [1 0]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[3 0]\n",
      " [1 1]\n",
      " [3 2]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [2 0]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "Test:\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [3 1]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]]\n",
      "Test:\n",
      "[[1 0]\n",
      " [3 2]\n",
      " [2 4]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rkf = RepeatedKFold(n_splits=3, n_repeats=5, random_state=2020)\n",
    "for train_index, test_index in rkf.split(X_data):\n",
    "      print(\"Train:\")\n",
    "      print(X_data[train_index])\n",
    "      print(\"Test:\")\n",
    "      print(X_data[test_index])\n",
    "      print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSNKDVAmch-q"
   },
   "source": [
    "### Stratified K-fold Cross Validation\n",
    "\n",
    "Stratified K-fold CV is particularly useful when the data is imbalanced. See below code for the use of the `StratifiedKFold` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oxo_6IWicD0K"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ddba54e6a358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# X is the feature set and y is the target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Validation:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_index' is not defined"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    print(\"Train:\", train_index, \"Validation:\", val_index) \n",
    "    X_train, X_test = X[train_index], X[val_index] \n",
    "    y_train, y_test = y[train_index], y[val_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQgLrZ3ChwxS"
   },
   "source": [
    "As said above, for model evaluation purposes, we can simply use the `cross_val_score` function. In the `cross_val_score` function, if the `cv` value is _integer_, the model (e.g. `clf`) is a classifier, and `y` is _categorical_ (e.g. _binary_ in this case),  `StratifiedKFold` is used. In all other cases, `KFold` is used. In short, `cross_val_score` by default apply stratified K-fold CV for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_14GHy2yi2ju"
   },
   "source": [
    "### Cross Validation for Time Series\n",
    "\n",
    "Time series data is very special - since the time sequence is implied in the data. Thus, splitting a time-series dataset randomly does not work because the time section of your data will be messed up. For a time series forecasting problem, we perform cross validation in the forward chaining manner.\n",
    "\n",
    "```\n",
    "fold 1: training [1], test [2]\n",
    "fold 2: training [1 2], test [3]\n",
    "fold 3: training [1 2 3], test [4]\n",
    "fold 4: training [1 2 3 4], test [5]\n",
    "fold 5: training [1 2 3 4 5], test [6]\n",
    ".\n",
    ".\n",
    ".\n",
    "fold n: training [1 2 3 ….. n-1], test [n]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZTrRe-Jiy66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]]\n",
      "Test:\n",
      "[[2 0]\n",
      " [1 1]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]]\n",
      "Test:\n",
      "[[3 1]\n",
      " [1 0]]\n",
      "\n",
      "\n",
      "Train:\n",
      "[[1 4]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]]\n",
      "Test:\n",
      "[[3 2]\n",
      " [2 4]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_index, test_index in tscv.split(X_data):\n",
    "      print(\"Train:\")\n",
    "      print(X_data[train_index])\n",
    "      print(\"Test:\")\n",
    "      print(X_data[test_index])\n",
    "      print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kojjoc4ej96l"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this tutorial, we discussed the importance of cross validation in machine learning models, in particular we focused on the __model evaluation__ use of CV. Thus, you should be comfortable using the `cross_val_score` function in your model evaluation phase.\n",
    "\n",
    "We also surveyed the most popular CV methods - `sklearn` support most of them natively. In other projects, you may want to use other CV methods.\n",
    "\n",
    "In next week's tutorial, we will use CV for another important purpose: model optimization. Till then, try CV on your own to evaluate your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Cross_Validation_in_Python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
