{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4BYyYQHeP7G"
   },
   "source": [
    "# Hyperparameter Optimization in Machine Learning Models \n",
    "This tutorial covers what a parameter and a hyperparameter are in a machine learning model along with why it is vital in order to enhance your model’s performance.\n",
    "Machine learning involves predicting and classifying data and to do so, you employ various machine learning models according to the dataset. Machine learning models are **parameterized** so that their behavior can be **tuned** for a given problem. These models can have many parameters and finding the best combination of parameters can be treated as a search problem. But this very term called parameter may appear unfamiliar to you if you are new to applied machine learning. But don’t worry! You will get to know about it in the very first place of this blog, and you will also discover what the difference between a **parameter** and a **hyperparameter** of a machine learning model is. This blog consists of following sections:\n",
    "\n",
    "- What are a parameter and a hyperparameter in a machine learning model?\n",
    "- Why hyperparameter optimization/tuning is vital in order to enhance your model’s performance?\n",
    "- Two simple strategies to optimize/tune the hyperparameters\n",
    "- A simple case study in Python with the two strategies\n",
    "\n",
    "Let’s straight jump into the first section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSy69ixheP7H"
   },
   "source": [
    "## What is a parameter in a machine learning learning model?\n",
    "A model parameter is a **configuration variable** that is internal to the model and whose value can be estimated from the given data.\n",
    "\n",
    "- They are required by the model when making predictions.\n",
    "- Their values define the skill of the model on your problem.\n",
    "- They are estimated or learned from data.\n",
    "- They are often not set manually by the practitioner.\n",
    "- They are often saved as part of the learned model.\n",
    "\n",
    "So your main take away from the above points should be parameters are crucial to machine learning algorithms. Also, they are the part of the model that is **learned from historical training data**. Let’s dig it a bit deeper. Think of the function parameters that you use while programming in general. You may pass a parameter to a function. In this case, a parameter is a function argument that could have one of a range of values. In machine learning, the specific model you are using is the function and requires parameters in order to make a prediction on new data. Whether a model has a fixed or variable number of parameters determines whether it may be referred to as “parametric” or “nonparametric“. \n",
    "\n",
    "Some examples of model parameters include:\n",
    "- The weights in an artificial neural network.\n",
    "- The support vectors in a support vector machine.\n",
    "- The coefficients in a linear regression or logistic regression.\n",
    "\n",
    "__NOTE__: Determining the best set of parameters is the essential step of training __any__ machine learning models. As you saw before, using the same modeling technique and the same training data, the model performance can be different - that is because that different parameters are learnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_osVnhYeP7I"
   },
   "source": [
    "## What is a hyperparameter in a machine learning learning model?\n",
    "A model hyperparameter is a configuration that is __external__ to the model and whose value __cannot be estimated__ from data.\n",
    "\n",
    "- They are often used in processes to help estimate model parameters.\n",
    "- They are often specified by the practitioner (manually).\n",
    "- They can often be set using heuristics (rules-of-thumb).\n",
    "- They are often tuned for a given predictive modeling problem (problem-specific).\n",
    "\n",
    "You cannot know the best value for a model hyperparameter on a given problem. You may use:\n",
    "1. rules of thumb, \n",
    "2. copy values used on other issues, \n",
    "3. or search for the best value by trial and error. \n",
    "\n",
    "When a machine learning algorithm is tuned for a specific problem then essentially you are tuning the hyperparameters of the model to discover the parameters of the model that result in the most skillful predictions. \n",
    "\n",
    "According to a very popular book called “Applied Predictive Modelling” - “Many models have important parameters which cannot be directly estimated from the data. For example, in the K-nearest neighbor classification model … This type of model parameter is referred to as a tuning parameter because there is no analytical formula available to calculate an appropriate value.” \n",
    "\n",
    "Model hyperparameters are often referred to as model parameters which can make things confusing. A good rule of thumb to overcome this confusion is as follows: “If you have to specify a model parameter **manually**, then it is probably a model hyperparameter. ” Some examples of model hyperparameters include:\n",
    "\n",
    "- The learning rate for training a neural network.\n",
    "- The C and sigma hyperparameters for support vector machines.\n",
    "- The k in k-nearest neighbors.\n",
    "\n",
    "In the next section, you will discover the importance of the right set of hyperparameter values in a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-9pOPB_eP7I"
   },
   "source": [
    "## Importance of the right set of hyperparameter values in a machine learning model:\n",
    "\n",
    "The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance, just as you might *turn the knobs of an AM radio to get a clear signal*. When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often, you don't immediately know what the **optimal** model architecture should be for a given model, and thus you'd like to be able to explore a range of possibilities. In a true machine learning fashion, you’ll ideally ask the machine to perform this exploration and select the optimal model architecture automatically.\n",
    "\n",
    "__NOTE__: Do not forget feature engineering, aside from hyperparameter optimization, is another way of tweaking models for better performances.\n",
    "\n",
    "You will see in the case study section on how the right choice of hyperparameter values affect the performance of a machine learning model. In this context, choosing the right set of values is typically known as “Hyperparameter optimization” or “Hyperparameter tuning”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "egftl2gQeP7J"
   },
   "source": [
    "## Two simple strategies to optimize/tune the hyperparameters:\n",
    "\n",
    "Models can have many hyperparameters and finding the best combination of parameters can be treated as a search problem.\n",
    "\n",
    "Although there are many hyperparameter optimization/tuning algorithms now, this post discusses two simple strategies: \n",
    "\n",
    "1. grid search and \n",
    "2. Random Search.\n",
    "\n",
    "### Grid searching of hyperparameters:\n",
    "Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. \n",
    "\n",
    "Let’s consider the following example: \n",
    "\n",
    "Suppose, a machine learning model `X` takes hyperparameters `a1`, `a2` and `a3.` In grid searching, you first define the range of values for each of the hyperparameters `a1`, `a2` and `a3`. You can think of this as an array of values for each of the hyperparameters. __NOTE__ that the ranges are usually heuristic. Now the grid search technique will construct many versions of `X` with all the possible combinations of hyperparameter (`a1`, `a2` and `a3`) values that you defined in the first place. This range of hyperparameter values is referred to as the grid. \n",
    "\n",
    "Suppose, you defined the grid as - __Note__ that usually the range of a hyperparameter is NOT continuous:\n",
    "`a1 = [0,1,2,3,4,5]`\n",
    "`a2 = [10,20,30,40,50,60]`\n",
    "`a3 = [105,105,110,115,120,125]`\n",
    "\n",
    "Note that, the array of values of that you are defining for the hyperparameters has to be legitimate in a sense that you cannot supply Floating type values to the array if the hyperparameter only takes Integer values.\n",
    "\n",
    "Now, grid search will begin its process of constructing several versions of X with the grid that you just defined.\n",
    "\n",
    "It will start with the combination of `[0,10,105]`, and it will end with `[5,60,125]`. It will go through all the intermediate combinations between these two which makes grid search computationally very expensive.\n",
    "\n",
    "Let’s take a look at the other search technique Random search:\n",
    "\n",
    "### Random searching of hyperparameters:\n",
    "The idea of random searching of hyperparameters was proposed by James Bergstra & Yoshua Bengio. You can check the original paper [here](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).\n",
    "\n",
    "Random search differs from a grid search. In that you longer provide a discrete set of values to explore for each hyperparameter; rather, you provide a statistical distribution for each hyperparameter from which values may be randomly sampled.\n",
    "\n",
    "Before going any further, let’s understand what distribution and sampling mean:\n",
    "\n",
    "In Statistics, by distribution, it is essentially meant an arrangement of values of a variable showing their observed or theoretical frequency of occurrence.\n",
    "\n",
    "On the other hand, Sampling is a term used in statistics. It is the process of choosing a representative sample from a target population and collecting data from that sample in order to understand something about the population as a whole.\n",
    "\n",
    "Now let's again get back to the concept of random search.\n",
    "\n",
    "You’ll define a sampling distribution for each hyperparameter. You can also define how many iterations you’d like to build when searching for the optimal model. For each iteration, the hyperparameter values of the model will be set by sampling the defined distributions. One of the primary theoretical backings to motivate the use of a random search in place of grid search is the fact that for most cases, hyperparameters are __not equally important__. According to the original paper:\n",
    "\n",
    "“*….for most datasets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different datasets. This phenomenon makes grid search a poor choice for configuring algorithms for new datasets.*”\n",
    "\n",
    "In the following figure, we're searching over a hyperparameter space where the one hyperparameter has significantly more influence on optimizing the model score - the distributions shown on each axis represent the model's score. In each case, we're evaluating `9` different models. The grid search strategy blatantly misses the optimal model and spends redundant time exploring the unimportant parameter. During this grid search, we isolated each hyperparameter and searched for the best possible value while holding all other hyperparameters constant. For cases where the hyperparameter being studied has little effect on the resulting model score, this results in wasted effort. Conversely, the random search has much improved exploratory power and can focus on finding the optimal value for the critical hyperparameter.\n",
    "\n",
    "<img src = 'http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531340388/grid_vs_random_jltknd.png' />\n",
    "\n",
    "Source: [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "\n",
    "In the following sections, you will see grid search and random search in action with Python. You will also be able to decide which is better regarding the effectiveness and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fb1I63N7eP7J"
   },
   "source": [
    "## Case study in Python:\n",
    "\n",
    "Hyperparameter tuning is a final step in the process of applied machine learning before presenting results.\n",
    "\n",
    "You will use the Pima Indian diabetes dataset. The dataset corresponds to a classification problem on which you need to make predictions on the basis of whether a person is to suffer diabetes given the `8` features in the dataset. You can find the complete description of the dataset [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database).\n",
    "\n",
    "There are a total of `768` observations in the dataset. Your first task is to load the dataset so that you can proceed. But before that let's import the dependencies you are going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71sIIgkFeP7J"
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM2rT5wyeP7M"
   },
   "source": [
    "Now that the dependencies are imported let's load Pima Indians dataset into a Dataframe object with the famous `Pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xNwAQIOeP7M"
   },
   "outputs": [],
   "source": [
    " data = pd.read_csv(\"./data/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKvMzr3LeP7O"
   },
   "source": [
    "The dataset is successfully loaded into the Dataframe object data. Now, let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_EeyWWPteP7P"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQhUt_JseP7S"
   },
   "source": [
    "So you can `8` different features labeled into the outcomes of `1` and `0` where `1` stands for the observation has diabetes, and `0` denotes the observation does not have diabetes. The dataset is known to have missing values. Specifically, there are missing observations for some columns that are marked as a zero value. We can corroborate this by the definition of those columns, and the domain knowledge that a zero value is invalid for those measures, e.g., `0` for body mass index or blood pressure is invalid.\n",
    "\n",
    "(Missing value creates a lot of problems when you try to build a machine learning model. In this case, you will use a Logistic Regression classifier for predicting the patients having diabetes or not. Now, Logistic Regression cannot handle the problems of missing values. )\n",
    "\n",
    "(If you want a quick refresher on Logistic Regression you can refer [here](https://www.analyticsvidhya.com/blog/2015/10/basics-logistic-regression/).)\n",
    "\n",
    "Let's get some statistics about the data with Pandas' `describe()` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ybiFo-JeP7S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all columns have a min of 0 except DiabetesPedigreeFunction and Age.. is this valid?\n",
    "# all the columns have a count of 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gwz5rSBueP7U"
   },
   "source": [
    "This is useful.\n",
    "\n",
    "We can see that there are columns that have a minimum value of `0`. On some columns, a value of `0` does not make sense and indicates an invalid or missing value.\n",
    "\n",
    "Specifically, the following columns have an invalid zero minimum value:\n",
    "\n",
    "- Plasma glucose concentration (`Glucose`)\n",
    "- Diastolic blood pressure (`BloodPressure`)\n",
    "- Triceps skinfold thickness (`SkinThickness`)\n",
    "- 2-Hour serum insulin (`Insulin`)\n",
    "- Body mass index (`BMI`)\n",
    "\n",
    "Now you need to identify and mark values as missing. Let’s confirm this by looking at the raw data, the example prints the first `20` rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAQCahiFeP7U"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0             6      148             72             35        0  33.6   \n",
       "1             1       85             66             29        0  26.6   \n",
       "2             8      183             64              0        0  23.3   \n",
       "3             1       89             66             23       94  28.1   \n",
       "4             0      137             40             35      168  43.1   \n",
       "5             5      116             74              0        0  25.6   \n",
       "6             3       78             50             32       88  31.0   \n",
       "7            10      115              0              0        0  35.3   \n",
       "8             2      197             70             45      543  30.5   \n",
       "9             8      125             96              0        0   0.0   \n",
       "10            4      110             92              0        0  37.6   \n",
       "11           10      168             74              0        0  38.0   \n",
       "12           10      139             80              0        0  27.1   \n",
       "13            1      189             60             23      846  30.1   \n",
       "14            5      166             72             19      175  25.8   \n",
       "15            7      100              0              0        0  30.0   \n",
       "16            0      118             84             47      230  45.8   \n",
       "17            7      107             74              0        0  29.6   \n",
       "18            1      103             30             38       83  43.3   \n",
       "19            1      115             70             30       96  34.6   \n",
       "\n",
       "    DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                      0.627   50        1  \n",
       "1                      0.351   31        0  \n",
       "2                      0.672   32        1  \n",
       "3                      0.167   21        0  \n",
       "4                      2.288   33        1  \n",
       "5                      0.201   30        0  \n",
       "6                      0.248   26        1  \n",
       "7                      0.134   29        0  \n",
       "8                      0.158   53        1  \n",
       "9                      0.232   54        1  \n",
       "10                     0.191   30        0  \n",
       "11                     0.537   34        1  \n",
       "12                     1.441   57        0  \n",
       "13                     0.398   59        1  \n",
       "14                     0.587   51        1  \n",
       "15                     0.484   32        1  \n",
       "16                     0.551   31        1  \n",
       "17                     0.254   31        1  \n",
       "18                     0.183   33        0  \n",
       "19                     0.529   32        1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ej6dH1yYeP7X"
   },
   "source": [
    "You can see 0 in several columns, right?\n",
    "\n",
    "You can get a count of the number of missing values in each of these columns. You can do this by marking all of the values in the subset of the DataFrame you are interested in that have zero values as True. You can then count the number of true values in each column. For this, you will have to reimport the data without the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_pGhXmmeP7X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\",header=None)\n",
    "print((data[[1,2,3,4,5]] == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eW9o6AnpeP7Z"
   },
   "source": [
    "You can see that columns `1`,`2` and `5` have just a few `0` values, whereas columns `3` and `4` show a lot more, nearly half of the rows. Column `0` has several missing values although but that is natural. Column `8` denotes the target variable so, `0`s in it is natural.\n",
    "\n",
    "This highlights that different “missing value” strategies may be needed for different columns, e.g., to ensure that there are still a sufficient number of records left to train a predictive model.\n",
    "\n",
    "In Python, specifically `Pandas`, `NumPy` and `Scikit-Learn`, you should mark missing values as `NaN`.\n",
    "\n",
    "Values with a `NaN` value are ignored from operations like sum, count, etc.\n",
    "\n",
    "You can mark values as NaN easily with the Pandas DataFrame by using the replace() function on a subset of the columns you are interested in.\n",
    "\n",
    "After you have marked the missing values, you can use the `isnull()` function to mark all of the NaN values in the dataset as `True` and get a count of the missing values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS6k_BX9eP7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      5\n",
      "2     35\n",
      "3    227\n",
      "4    374\n",
      "5     11\n",
      "6      0\n",
      "7      0\n",
      "8      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mark zero values as missing or NaN\n",
    "data[[1,2,3,4,5]] = data[[1,2,3,4,5]].replace(0, np.NaN)\n",
    "# Count the number of NaN values in each column\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVIeSp9qeP7b"
   },
   "source": [
    "You can see that the columns `1` to `5` have the same number of missing values as zero values identified above. This is a sign that you have marked the identified missing values correctly.\n",
    "\n",
    "This is a useful summary. But you'd like to look at the actual data though, to confirm that you have not fooled yourselves.\n",
    "\n",
    "Below is the same example, except you print the first `5` rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "feXKteVVeP7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1     2     3      4     5      6   7  8\n",
       "0  6  148.0  72.0  35.0    NaN  33.6  0.627  50  1\n",
       "1  1   85.0  66.0  29.0    NaN  26.6  0.351  31  0\n",
       "2  8  183.0  64.0   NaN    NaN  23.3  0.672  32  1\n",
       "3  1   89.0  66.0  23.0   94.0  28.1  0.167  21  0\n",
       "4  0  137.0  40.0  35.0  168.0  43.1  2.288  33  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEMV1GL7eP7e"
   },
   "source": [
    "It is clear from the raw data that marking the missing values had the intended effect. Now, you will **impute** the missing values. Imputing refers to using a model to replace missing values. Although there are several solutions for imputing missing values, you will use mean imputation which means replacing the missing values in a column with the mean of that particular column. Let's do this with `Pandas`' `fillna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UawNFUA0eP7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with mean column values\n",
    "data.fillna(data.median(), inplace=True)\n",
    "# Count the number of NaN values in each column\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dcwczy5NeP7h"
   },
   "source": [
    "Cheers! You have now handled the missing value problem. Now let's use this data to build a Logistic Regression model using `scikit-learn`.\n",
    "\n",
    "First, you will see the model with some random hyperparameter values. Then you will build two other Logistic Regression models with two different strategies - **Grid search** and **Random search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atjvUDRBeP7h"
   },
   "outputs": [],
   "source": [
    "# Split dataset into inputs and outputs\n",
    "values = data.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]\n",
    "\n",
    "# Initiate the LR model with random hyperparameters\n",
    "lr = LogisticRegression(penalty='l2',dual=False,max_iter=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgmnMh_0eP7j"
   },
   "source": [
    "You have created the Logistic Regression model with some random hyperparameter values. The hyperparameters that you used are:\n",
    "\n",
    "- `penalty`: Used to specify the norm used in the penalization ([regularization](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)).\n",
    "- `dual`: Dual or primal formulation. The dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features (when you have more observations than features).\n",
    "- `max_iter`: Maximum number of iterations taken to converge.\n",
    "\n",
    "Later in the case study, you will optimize/tune these hyperparameters so see the change in the results.\n",
    "\n",
    "__NOTE__: in this tutorial, our main purpose is to optimize hyperparameters - hence we did not split the data into training/testing. But you should always split your data. However later on we applied K-fold cross validation to ensure the model(s) is not overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ojQ-l7meP7j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=110,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass data to the LR model\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lbaUEQPeP7n"
   },
   "source": [
    "It's time to check the __accuracy__ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RK_jwZHeP7n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7760416666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nblf6tjeP7p"
   },
   "source": [
    "In the above step, you applied your LR model to the same data and evaluated its score. But there is always a need to validate the stability of your machine learning model. You just can’t fit the model to your training data and hope it would accurately work for the real data it has never seen before. You need some kind of assurance that your model has got most of the patterns from the data correct.\n",
    "\n",
    "Well, Cross-validation is there for rescue. I will not go into the details of it as it is out of the scope of this blog. But [this post](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f) does a very fine job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1f9BuLteP7p"
   },
   "outputs": [],
   "source": [
    "# You will need the following dependencies for applying Cross-validation and evaluating the cross-validated score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Build the k-fold cross-validator\n",
    "\n",
    "kfold = KFold(n_splits=3, random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBvBMSWAeP7r"
   },
   "source": [
    "You supplied `n_splits` as `3`, which essentially makes it a 3-fold cross-validation.\n",
    "\n",
    "__NOTE__: this is because that you have a limited-sized dataset. \n",
    "\n",
    "You also supplied `random_state` as 2019. This is just to reproduce the results. You could have supplied any integer value as well. \n",
    "\n",
    "__NOTE__: you should always ensure the __reproducibility__ of your data analysis.\n",
    "\n",
    "Now, let's apply this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzqSoWjZeP7s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7643229166666666\n"
     ]
    }
   ],
   "source": [
    "result = cross_val_score(lr, X, y, cv=kfold, scoring='accuracy')\n",
    "print(result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ASExQk1eP7u"
   },
   "source": [
    "You can see there's a slight decrease in the score. This is because you are usng less (**66%**) of your data to train the model. Anyway, you can do better with hyperparameter tuning/optimization.\n",
    "\n",
    "Let's build another LR model, but this time its hyperparameter will be tuned. You will first do this grid search.\n",
    "\n",
    "Let's first import the dependencies you will need. Scikit-learn provides a utility called `GridSearchCV` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lrxuvC-eP7u"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rz6jUCAheP7w"
   },
   "source": [
    "Let's define the grid values of the hyperparameters that you used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnUT4-gHeP7w"
   },
   "outputs": [],
   "source": [
    "dual=[True,False]\n",
    "max_iter=[100,110,120,130,140]\n",
    "param_grid = dict(dual=dual,max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CStvUzKleP7y"
   },
   "source": [
    "You have defined the grid. Let's run the grid search over them and see the results with execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZSstD2peP7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.769531 using {'dual': False, 'max_iter': 130}\n",
      "Execution time: 2.973355293273926 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 3, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_result = grid.fit(X, y)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iW-ATr6IeP71"
   },
   "source": [
    "Oh-no! The results decreased again! Does that mean we are totally off track? Rest assured, this only means that the current grid does not contain the optimal set.\n",
    "\n",
    "You can define a larger grid of hyperparameter as well and apply grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWqeIwKWeP73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.769531 using {'dual': False, 'max_iter': 130}\n",
      "Execution time: 0.11803579330444336 ms\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2')\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 3, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_result = grid.fit(X, y)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4790gPtieP75"
   },
   "source": [
    "You can see an increase in the accuracy score, but there is a sufficient amount of growth in the execution time as well. **The larger the grid, the more execution time**.\n",
    "\n",
    "Let's rerun everything but this time with the random search. Scikit-learn provides `RandomSearchCV` to do that. As usual, you will have to import the necessary dependencies for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJSfiUpKeP75"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0W_rzotZkmuP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "max_iter1 = np.random.randint(100, 200, 50)\n",
    "C1 = np.random.uniform(0, 5, 50)\n",
    "param_grid1 = dict(dual=dual,max_iter=max_iter1,C=C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38jovF-peP77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.766927 using {'max_iter': 125, 'dual': False, 'C': 4.7699570303085705}\n",
      "Execution time: 0.12347030639648438 ms\n"
     ]
    }
   ],
   "source": [
    "random = RandomizedSearchCV(estimator=lr, param_distributions=param_grid1, cv = 3, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "random_result = random.fit(X, y)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlfHzpg4eP79"
   },
   "source": [
    "Woah! The random search yielded the same accuracy but in a much lesser time.\n",
    "\n",
    "#### YOUR TURN HERE\n",
    "Please check the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) for `RandomSearchCV`, for the purpose of understanding why we set `n_jobs` to `-1`.\n",
    "\n",
    "Also, please list any other random search parameters that you found interesting in the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXo2MjeZeP79"
   },
   "source": [
    "- n_jobs = The number of jobs to run in parallel...\n",
    "    - None means 1 unless in a joblib.parallel_backend context\n",
    "    - 1 means using all processors\n",
    "- pre_dispatch = Controls the number of jobs that get dispatched during parallel execution. \n",
    "    - Can help to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. \n",
    "    - Can be none, and int, or a string\n",
    "- refit = Refit an estimator using the best found parameters on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_effwwSMeP79"
   },
   "source": [
    "That is all for the case study part. Now, let's wrap things up!\n",
    "\n",
    "## Conclusion and further reading:\n",
    "In this tutorial, you learned about parameters and hyperparameters of a machine learning model and their differences as well. You also got to know about what role hyperparameter optimization plays in building efficient machine learning models. You built a simple Logistic Regression classifier in Python with the help of scikit-learn.\n",
    "\n",
    "You tuned the hyperparameters with grid search and random search and saw which one performs better.\n",
    "\n",
    "Besides, you saw small data preprocessing steps (like handling missing values) that are required before you feed your data into the machine learning model. You covered Cross-validation as well.\n",
    "\n",
    "That is a lot to take in, and all of them are equally important in your data science journey. I will leave you with some further readings that you can do.\n",
    "\n",
    "Further readings:\n",
    "\n",
    "- [Problems in hyperparameter optimization](https://blog.sigopt.com/posts/common-problems-in-hyperparameter-optimization)\n",
    "- [Hyperparameter optimization with soft computing techniques](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n",
    "- [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)\n",
    "\n",
    "For the ones who are a bit more advanced, I would highly recommend reading this paper for effectively [optimizing the hyperparameters of neural networks](https://arxiv.org/abs/1803.09820)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_E4PJ0TOhFtX"
   },
   "source": [
    "# Bonus Content: Introducing the Implementation of a Pipeline\n",
    "\n",
    "We have been using the concept of a __pipeline__ frequently in this class. A __pipeline__ refers to a sequence of tasks covering one or more phases in [CRISP-DM](http://www.proglobalbusinesssolutions.com/six-steps-in-crisp-dm-the-standard-data-mining-process/). \n",
    "\n",
    "Even though we embraced the idea of pipelines, we have not formally implemented the idea of a pipeline in `sklearn`. The implementation so far from us is basically stacking different steps together. \n",
    "\n",
    "For instance, we observe that the dataset used in this tutorial contains `NaN` values, we will need to __impute__ them.  Additionally, since the data is not at the same range, natrually we would like to __scale__ the data.  After these steps, we want to perform __grid search__ (with cross validation) on the __logistic regression model__ to find the optimal hyperparameter. Then we __build the best model__ with the optimal hyperparameter settings to get the results. \n",
    "\n",
    "So, the aforementioned steps form a pipeline like below:\n",
    "```\n",
    "INPUT: Raw Data \n",
    "---------------\n",
    "STEP 1: SimpleImputer\n",
    "STEP 2: StandardScaler\n",
    "STEP 3: GridSearchCV + LogisticRegression\n",
    "STEP 4: LogisticRegression.predict\n",
    "----------------\n",
    "OUTPUT: Model Evaluation Results\n",
    "```\n",
    "As of now, we would implement the pipeline like this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-_UM7HJeP7-"
   },
   "outputs": [],
   "source": [
    "data_copy = data.copy()\n",
    "values = data_copy.values\n",
    "X = values[:,0:8]\n",
    "y = values[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2QQdpdszgsc"
   },
   "outputs": [],
   "source": [
    "# imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputedData = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tbb1cGNtz_VY"
   },
   "outputs": [],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaledData = scaler.fit_transform(imputedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D11XCz040Oq5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.768229 using {'C': 1.0, 'dual': False, 'max_iter': 100}\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# hyper parameters\n",
    "dual=[True,False]\n",
    "max_iter=[100,110,120,130,140]\n",
    "C = [1.0,1.5,2.0,2.5]\n",
    "param_grid = dict(dual=dual,max_iter=max_iter,C=C)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 3, n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(scaledData, y)\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNHZlUyS0_J8"
   },
   "source": [
    "We just saw this process in the main body of the tutorial. The only step left is to __fit__ and __evaluate__ the best model. If you have questions about this step, refer to the `Cross Validation in Python` tutorial from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3CRAQyn06s6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model bias (f1-score): 0.6311282922213759\n",
      "model variance (f1-score): 0.07077199665529102\n",
      "model bias (ROC/AUC): 0.8383526232081415\n",
      "model variance (ROC/AUC): 0.04032256379521683\n"
     ]
    }
   ],
   "source": [
    "# fit & evaluate best model\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "lr_best = LogisticRegression(C=1.0, dual=False, max_iter=100)\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=5, random_state=2020)\n",
    "f1_scores_cv = cross_val_score(lr_best, scaledData, y, scoring='f1', cv=rkf)\n",
    "roc_auc_cv = cross_val_score(lr_best, scaledData, y, scoring='roc_auc', cv=rkf)\n",
    "\n",
    "print(\"model bias (f1-score):\", f1_scores_cv.mean())\n",
    "print(\"model variance (f1-score):\", f1_scores_cv.std())\n",
    "print(\"model bias (ROC/AUC):\", roc_auc_cv.mean())\n",
    "print(\"model variance (ROC/AUC):\", roc_auc_cv.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGmkQrJf3bfj"
   },
   "source": [
    "To prove that this is the best hyperparameter setting, we can create a model for compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_E-ccMbZ2esL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model bias (f1-score): 0.6228013069015836\n",
      "model bias (ROC/AUC): 0.8391847658903085\n"
     ]
    }
   ],
   "source": [
    "# model for compare\n",
    "lr_comp = LogisticRegression(C=0.1, dual=False, max_iter=50)\n",
    "f1_scores_comp = cross_val_score(lr_comp, scaledData, y, scoring='f1', cv=rkf)\n",
    "roc_auc_comp = cross_val_score(lr_comp, scaledData, y, scoring='roc_auc', cv=rkf)\n",
    "\n",
    "print(\"model bias (f1-score):\", f1_scores_comp.mean())\n",
    "print(\"model bias (ROC/AUC):\", roc_auc_comp.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOPLd23e5BM-"
   },
   "source": [
    "Since we have `50` values in both model evaluation results, we can perform the _t-test_ for comparison purpose.\n",
    "\n",
    "The function below (original code from [here](https://towardsdatascience.com/inferential-statistics-series-t-test-using-numpy-2718f8f9bf2f)) returns the _t-test_ results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Baf4RrD_4aob"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def my_t_test(a,b,size): # a, b are two arrays for comparison\n",
    "    n = size # size of the arrays\n",
    "\n",
    "    ## Calculate the Standard Deviation\n",
    "    var_a = a.var(ddof=1)\n",
    "    var_b = b.var(ddof=1)\n",
    "\n",
    "    #std deviation\n",
    "    s = np.sqrt((var_a + var_b)/2)\n",
    "\n",
    "    ## Calculate the t-statistics\n",
    "    t = (a.mean() - b.mean())/(s*np.sqrt(2/n))\n",
    "\n",
    "    ## Compare with the critical t-value\n",
    "    #Degrees of freedom\n",
    "    df = 2*n - 2\n",
    "\n",
    "    #p-value after comparison with the t \n",
    "    p = 1 - stats.t.cdf(t,df=df)  \n",
    "  \n",
    "  \n",
    "    # return(t, p)\n",
    "\n",
    "    if (abs(t) > 1.96) and p < 0.05:\n",
    "        return('mean of a and b are significantly different (reject H0)')\n",
    "    else:\n",
    "        return('mean of a and b are not significantly different (not reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfHTpdAr6R39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean of a and b are not significantly different (not reject H0)'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_t_test(f1_scores_cv, f1_scores_comp, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqqGEZQR6W7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean of a and b are not significantly different (not reject H0)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_t_test(roc_auc_cv, roc_auc_comp, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VIHF7pT47E_0"
   },
   "source": [
    "Above results prove that the \"best\" model is not significantly different from a random model we create for comparison. And the last part (__model evaluation and comparison__) ties back to the tutorial from last week.\n",
    "\n",
    "`sklearn` formally support the idea of pipelines, we can easily combine STEPs 1 - 3 in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3Y-44lh6cGH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model bias (f1-score): 0.6311282922213759\n",
      "model variance (f1-score): 0.07077199665529102\n",
      "model bias (ROC/AUC): 0.8383251988691156\n",
      "model variance (ROC/AUC): 0.04031707644766859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# STEP 1 - 3\n",
    "best_pipeline = make_pipeline(imputer, # STEP 1\n",
    "                    scaler, # STEP 2\n",
    "                    GridSearchCV(LogisticRegression(penalty='l2'),\n",
    "                                 param_grid=param_grid,\n",
    "                                 cv=3)) # STEP 3\n",
    "\n",
    "# STEP 4\n",
    "f1_scores_pp = cross_val_score(best_pipeline, scaledData, y, scoring='f1', cv=rkf)\n",
    "roc_auc_pp = cross_val_score(best_pipeline, scaledData, y, scoring='roc_auc', cv=rkf)\n",
    "\n",
    "print(\"model bias (f1-score):\", f1_scores_pp.mean())\n",
    "print(\"model variance (f1-score):\", f1_scores_pp.std())\n",
    "print(\"model bias (ROC/AUC):\", roc_auc_pp.mean())\n",
    "print(\"model variance (ROC/AUC):\", roc_auc_pp.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iyYZOT46-K3U"
   },
   "source": [
    "See how elegant your pipeline can be? You can even incorporate your custom functions in there.\n",
    "\n",
    "Hopefully this part is helpful when you organize your pipelines."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qlfHzpg4eP79",
    "_effwwSMeP79"
   ],
   "name": "Hyperparameter_Optimization_in_ML_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
